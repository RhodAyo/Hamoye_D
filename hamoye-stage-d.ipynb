{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-22T22:07:07.102324Z","iopub.execute_input":"2022-03-22T22:07:07.102647Z","iopub.status.idle":"2022-03-22T22:07:58.924836Z","shell.execute_reply.started":"2022-03-22T22:07:07.102564Z","shell.execute_reply":"2022-03-22T22:07:58.919395Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"ID: 1485ba305ac1f000","metadata":{}},{"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport numpy as np\nfrom os import listdir\nfrom numpy import zeros\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom pandas import read_csv\nfrom keras import backend\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nimport os\nimport time\nfrom sklearn.metrics import fbeta_score\nimport tensorflow as tf\nimport sys\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Conv2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import load_model\n","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:20:38.876403Z","iopub.execute_input":"2022-03-22T22:20:38.877179Z","iopub.status.idle":"2022-03-22T22:20:39.310765Z","shell.execute_reply.started":"2022-03-22T22:20:38.877119Z","shell.execute_reply":"2022-03-22T22:20:39.309857Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Libraries for Data Visualisation\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom matplotlib.image import imread\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:21:19.834100Z","iopub.execute_input":"2022-03-22T22:21:19.834387Z","iopub.status.idle":"2022-03-22T22:21:19.841030Z","shell.execute_reply.started":"2022-03-22T22:21:19.834345Z","shell.execute_reply":"2022-03-22T22:21:19.840434Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:22:05.174914Z","iopub.execute_input":"2022-03-22T22:22:05.175702Z","iopub.status.idle":"2022-03-22T22:22:05.194765Z","shell.execute_reply.started":"2022-03-22T22:22:05.175666Z","shell.execute_reply":"2022-03-22T22:22:05.194022Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#import train_classes.csv, test and train jpg folder\ntrain_classes = pd.read_csv('../input/planet/planet/planet/train_classes.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:24:08.573643Z","iopub.execute_input":"2022-03-22T22:24:08.574190Z","iopub.status.idle":"2022-03-22T22:24:08.637959Z","shell.execute_reply.started":"2022-03-22T22:24:08.574155Z","shell.execute_reply":"2022-03-22T22:24:08.637397Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ntest_jpg_datasets = '../input/planet/planet/planet/test-jpg/'\ntrain_jpg_datasets = '../input/planet/planet/planet/train-jpg/'","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:25:29.858295Z","iopub.execute_input":"2022-03-22T22:25:29.858889Z","iopub.status.idle":"2022-03-22T22:25:29.863436Z","shell.execute_reply.started":"2022-03-22T22:25:29.858845Z","shell.execute_reply":"2022-03-22T22:25:29.862513Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#view train datasets\ntrain_classes.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:26:08.468959Z","iopub.execute_input":"2022-03-22T22:26:08.469286Z","iopub.status.idle":"2022-03-22T22:26:08.489013Z","shell.execute_reply.started":"2022-03-22T22:26:08.469255Z","shell.execute_reply":"2022-03-22T22:26:08.488454Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nfolder =  \"planet/train-jpg/\"\nfor i in range(12):\n    pyplot.subplot(330 + 1 + i)\n\n    file = train_jpg_datasets + 'train_' + str(i) + '.jpg'\n    \n    #Load pixels of the images\n    images = imread(file)\n    pyplot.imshow(images)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:30:06.943396Z","iopub.execute_input":"2022-03-22T22:30:06.943655Z","iopub.status.idle":"2022-03-22T22:30:08.816732Z","shell.execute_reply.started":"2022-03-22T22:30:06.943627Z","shell.execute_reply":"2022-03-22T22:30:08.815614Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"lists = []\nfor tag in train_classes.tags.values:\n    dataset = tag.split(' ')\n    for data in dataset:\n        if data not in lists:\n            lists.append(data)\n            \nlen(lists)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:36:02.081280Z","iopub.execute_input":"2022-03-22T22:36:02.081553Z","iopub.status.idle":"2022-03-22T22:36:02.130931Z","shell.execute_reply.started":"2022-03-22T22:36:02.081525Z","shell.execute_reply":"2022-03-22T22:36:02.130258Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"mapping = {lists[i]: i for i in range(len(lists))}\ninvert_mapping= {i:lists[i] for i in range(len(lists))}\ninvert_mapping","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:38:45.578616Z","iopub.execute_input":"2022-03-22T22:38:45.578902Z","iopub.status.idle":"2022-03-22T22:38:45.586185Z","shell.execute_reply.started":"2022-03-22T22:38:45.578870Z","shell.execute_reply":"2022-03-22T22:38:45.585592Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Map files to tags\ndef file_mapper(data_csv):\n    mapper = dict()\n    for i in range(len(data_csv)):\n        name, tags = data_csv['image_name'][i], data_csv['tags'][i]\n        mapper[name] = tags.split(' ')\n    return mapper\nmapped_file = file_mapper(train_classes)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:42:06.068717Z","iopub.execute_input":"2022-03-22T22:42:06.069496Z","iopub.status.idle":"2022-03-22T22:42:06.938961Z","shell.execute_reply.started":"2022-03-22T22:42:06.069448Z","shell.execute_reply":"2022-03-22T22:42:06.938125Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#function to categorize images using one_hot_encoder\ndef one_hot_encoder(tags, mapper):\n    \n    #Creating an empty matrix(Vector)\n    encoding = np.zeros(len(mapper), dtype='uint8')\n    \n    #Map 1 for each tag in the vector created\n    for t in tags:\n        encoding[mapper[t]] = 1\n    return encoding","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:43:03.585783Z","iopub.execute_input":"2022-03-22T22:43:03.586083Z","iopub.status.idle":"2022-03-22T22:43:03.591937Z","shell.execute_reply.started":"2022-03-22T22:43:03.586052Z","shell.execute_reply":"2022-03-22T22:43:03.591317Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def load_dataset(path, file_mapping, mapping):\n    photos = list()\n    targets = list()\n    \n    for file in os.listdir(train_jpg_datasets):\n        \n        #Load Images\n        photo = load_img(path + file, target_size = (64, 64))\n        \n        #Converting photos to numpy array\n        photo = img_to_array(photo, dtype = 'uint8')\n        \n        #Tags\n        tags = mapped_file[file[:-4]]\n        \n        target = one_hot_encoder(tags, tag_mapping)\n        \n        #Stores\n        photos.append(photo)\n        targets.append(target)\n    X = np.asarray(photos,dtype='uint8')\n    Y = np.asarray(targets,dtype='uint8')\n    return X,Y\n\nX, Y = load_dataset(train_jpg_datasets, mapped_file, mapping)\nprint(X.shape, Y.shape)\n\n# Compressing X and Y arrays into one single file\nnp.savez_compressed('testtraindata.zip', X, Y)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:47:50.958941Z","iopub.execute_input":"2022-03-22T22:47:50.959323Z","iopub.status.idle":"2022-03-22T22:51:53.045120Z","shell.execute_reply.started":"2022-03-22T22:47:50.959295Z","shell.execute_reply":"2022-03-22T22:51:53.044417Z"},"trusted":true},"execution_count":26,"outputs":[]}]}